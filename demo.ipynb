{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelechi/code/Machine_Learning/web_chat/.lc_venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\", google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "\n",
    "# Load documents\n",
    "loader = WebBaseLoader(\"https://www.anthropic.com/claude\")\n",
    "documents = loader.load()\n",
    "\n",
    "\n",
    "# Create vector store\n",
    "vector_store = Chroma.from_documents(documents, embeddings)\n",
    "\n",
    "# Initialize conversation memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "# Initialize language model\n",
    "llm = ChatAnthropic(\n",
    "    anthropic_api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "    model=\"claude-3-sonnet-20240229\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "# Create the chain\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm, vector_store.as_retriever(), memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsupported chat history format: <class 'str'>. Full chat history: Human: What is Anthropics lightest model?\nAI: According to the information provided, Anthropic's lightest and fastest model in the Claude family is called Haiku.\n\nThe context states:\n\n\"The Claude 3 family of models offers the best combination of speed and performance for enterprise use cases, at a lower cost than other models on the market.\n\nLight & fast\nHaiku\nOur fastest model that can execute lightweight actions, with industry-leading speed.\"\n\nSo Haiku is described as Anthropic's fastest and lightest model for lightweight tasks requiring high speed. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m chat_history \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is Anthropics lightest model?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mqa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchat_history\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/code/Machine_Learning/web_chat/.lc_venv/lib/python3.10/site-packages/langchain/chains/base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m~/code/Machine_Learning/web_chat/.lc_venv/lib/python3.10/site-packages/langchain/chains/base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/code/Machine_Learning/web_chat/.lc_venv/lib/python3.10/site-packages/langchain/chains/conversational_retrieval/base.py:142\u001b[0m, in \u001b[0;36mBaseConversationalRetrievalChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    140\u001b[0m question \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    141\u001b[0m get_chat_history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_chat_history \u001b[38;5;129;01mor\u001b[39;00m _get_chat_history\n\u001b[0;32m--> 142\u001b[0m chat_history_str \u001b[38;5;241m=\u001b[39m \u001b[43mget_chat_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchat_history\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chat_history_str:\n\u001b[1;32m    145\u001b[0m     callbacks \u001b[38;5;241m=\u001b[39m _run_manager\u001b[38;5;241m.\u001b[39mget_child()\n",
      "File \u001b[0;32m~/code/Machine_Learning/web_chat/.lc_venv/lib/python3.10/site-packages/langchain/chains/conversational_retrieval/base.py:50\u001b[0m, in \u001b[0;36m_get_chat_history\u001b[0;34m(chat_history)\u001b[0m\n\u001b[1;32m     48\u001b[0m         buffer \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([human, ai])\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     51\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported chat history format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(dialogue_turn)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Full chat history: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchat_history\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m         )\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m buffer\n",
      "\u001b[0;31mValueError\u001b[0m: Unsupported chat history format: <class 'str'>. Full chat history: Human: What is Anthropics lightest model?\nAI: According to the information provided, Anthropic's lightest and fastest model in the Claude family is called Haiku.\n\nThe context states:\n\n\"The Claude 3 family of models offers the best combination of speed and performance for enterprise use cases, at a lower cost than other models on the market.\n\nLight & fast\nHaiku\nOur fastest model that can execute lightweight actions, with industry-leading speed.\"\n\nSo Haiku is described as Anthropic's fastest and lightest model for lightweight tasks requiring high speed. "
     ]
    }
   ],
   "source": [
    "# Start the conversation\n",
    "chat_history = []\n",
    "query = \"What is Anthropics lightest model?\"\n",
    "result = qa.invoke({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result[\"answer\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.vectorstores import FAISS, Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os  # type: ignore\n",
    "\n",
    "# st.set_page_config(page_title=\"Web Chat\", page_icon=\":rocket:\")\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "base_llm = ChatAnthropic(\n",
    "    anthropic_api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "    model=\"claude-3-sonnet-20240229\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question based only on the the provided context:\n",
    "     <context>\n",
    "         {context}\n",
    "     </context>\n",
    "\n",
    "     Question: {input}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(base_llm, prompt)\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\", google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "\n",
    "\n",
    "def load_website_data(url):\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load()\n",
    "    documents = text_splitter.split_documents(docs)\n",
    "    vector = FAISS.from_documents(documents, embeddings)\n",
    "    retriever = vector.as_retriever()\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "# loader = WebBaseLoader(\"https://www.anthropic.com/claude\")\n",
    "# input_url = st.text_input(\"URL: \")\n",
    "\n",
    "\n",
    "# retriever = load_website_data('https://www.anthropic.com/claude')\n",
    "retriever = load_website_data(\"https://en.wikipedia.org/wiki/Perplexity.ai\")\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the Wikipedia article, the main information provided about Perplexity.ai is:\n",
      "\n",
      "1. It is an AI-powered research/conversational search engine that answers queries using natural language predictive text.\n",
      "\n",
      "2. It was launched in 2022 and generates answers using sources from the web, citing links within the text responses.\n",
      "\n",
      "3. It has a freemium model - the free version uses Perplexity's own model based on GPT-3.5, while the paid \"Perplexity Pro\" version has access to GPT-4, Claude 3, and other large language models.\n",
      "\n",
      "4. It was founded in August 2022 by Andy Konwinski, Denis Yarats, Johnny Ho, and Aravind Srinivas (the current CEO who previously worked at OpenAI).\n",
      "\n",
      "5. As of 2024, it has raised $100 million in funding valuing the company at $520 million, with investors including Jeff Bezos, Nvidia, Databricks and others.\n",
      "\n",
      "6. Its main functionality is providing personalized search results by interpreting the context of user queries and summarizing results with inline citations, prioritizing newer sources.\n",
      "\n",
      "So in summary, it covers Perplexity.ai's background as an AI search startup, its funding, founders, technology, product offerings and core search functionality.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the main infromations on the site?\"\n",
    "result = retrieval_chain.invoke({\"input\": query})\n",
    "\n",
    "print(result[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".lc_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
